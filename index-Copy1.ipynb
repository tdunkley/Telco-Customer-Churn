{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Student Name: Troy D. Dunkley\n",
    "* Student Pace: Full Time\n",
    "* Scheduled Project Review Date/Time: Monday 11.12.19\n",
    "* Instructors' Names: Amber Yandow and Howard Smith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRISP-DM Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Industry Standard Proces for Data Mining, also known as the CRISP-DM Methodology, is being employed for this project. It is an open standard process model that describes common approaches used by data mining experts. CRISP-DM is currently the dominant process framework for data mining. It is comprised of the following phases:\n",
    "\n",
    "* Business Understanding\n",
    "* Data Understanding\n",
    "* Data Preparation\n",
    "* Modeling\n",
    "* Evaluation\n",
    "* Deployment\n",
    "\n",
    "Below is a diagram of the methodology process flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <img src=\"customer-churn-1024x662.jpg\">\n",
    "# <img src=\"Cust_Churn_LiveBucket.png\">\n",
    "# <img src=\"customer-churn-1024x662.jpg\">\n",
    "# <img src=\"JetPack.png\">\n",
    "# <img src=\"Cust_Churn_Fish.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_pie(column) :\n",
    "    \n",
    "#     trace1 = go.Pie(values  = churn[column].value_counts().values.tolist(),\n",
    "#                     labels  = churn[column].value_counts().keys().tolist(),\n",
    "#                     hoverinfo = \"label+percent+name\",\n",
    "#                     domain  = dict(x = [0,.48]),\n",
    "#                     name    = \"Churn Customers\",\n",
    "#                     marker  = dict(line = dict(width = 2,\n",
    "#                                                color = \"rgb(243,243,243)\")\n",
    "#                                   ),\n",
    "#                     hole    = .6\n",
    "#                    )\n",
    "#     trace2 = go.Pie(values  = not_churn[column].value_counts().values.tolist(),\n",
    "#                     labels  = not_churn[column].value_counts().keys().tolist(),\n",
    "#                     hoverinfo = \"label+percent+name\",\n",
    "#                     marker  = dict(line = dict(width = 2,\n",
    "#                                                color = \"rgb(243,243,243)\")\n",
    "#                                   ),\n",
    "#                     domain  = dict(x = [.52,1]),\n",
    "#                     hole    = .6,\n",
    "#                     name    = \"Non churn customers\" \n",
    "#                    )\n",
    "\n",
    "\n",
    "#     layout = go.Layout(dict(title = column + \" distribution in customer attrition \",\n",
    "#                             plot_bgcolor  = \"rgb(243,243,243)\",\n",
    "#                             paper_bgcolor = \"rgb(243,243,243)\",\n",
    "#                             annotations = [dict(text = \"churn customers\",\n",
    "#                                                 font = dict(size = 13),\n",
    "#                                                 showarrow = False,\n",
    "#                                                 x = .15, y = .5),\n",
    "#                                            dict(text = \"Non churn customers\",\n",
    "#                                                 font = dict(size = 13),\n",
    "#                                                 showarrow = False,\n",
    "#                                                 x = .88,y = .5\n",
    "#                                                )\n",
    "#                                           ]\n",
    "#                            )\n",
    "#                       )\n",
    "#     data = [trace1,trace2]\n",
    "#     fig  = go.Figure(data = data,layout = layout)\n",
    "#     fig.show()\n",
    "    \n",
    "# for i in cat_cols :\n",
    "#     plot_pie(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Cust_Churn_LiveBucket.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Business Understanding Phase focuses on understanding the project objectives and requirements from a business perspective, and then converting this knowledge into a data mining problem definition and a preliminary plan. The primary tasks within this phase include the following:\n",
    "\n",
    "* Determine Business Objectives\n",
    "* Assess Situation\n",
    "* Determine Data Mining Goals\n",
    "* Produce Project Plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# According to the authors of “Leading on the Edge of Chaos”, a 2% increase in customer retention (or decreasing churn) is equivalent to 10% reduction in costs. so no wonder SaaS companies (and companies that care about customers) pay a lot of attention to Churn Analysis. Additionally as per the White House Office of Consumer Affairs, it is 6–7 times more expensive to acquire a new customer than to retain an old one.\n",
    "\n",
    "# they need a holistic churn management strategy thats takes into account risk (and associated risk tolerance), the level and cost of the retention intervention for different customer segments that is more systemic and continuous.\n",
    "\n",
    "\n",
    "# Churn could happen due to many different reasons and churn analysis helps to identify the cause (and timing) of this churn opening up opportunities to implement effective retention strategies. Here are 6 time-tested steps to make sure you are focusing on retaining your customers — we are going to focus only on step 2 and parts of step 3 for this article. While at this, remember that this is not about blaming the product or customer success group for the churn but to create a strategy to improve customer retention.\n",
    "\n",
    "# * Gather available customer behavior, transactions, demographics data and usage patterns\n",
    "# * Utilize these data points to predict customer segments who are likely to churn\n",
    "# * Create a model to pattern the risk tolerance of the business with respect to churn probability.\n",
    "# * Design an intervention model to consider how the level of intervention could affect the churn percentages and customer lifetime value (CLV)\n",
    "# * Implement effective experimentation across multiple customer segments for reducing churn and promoting retention.\n",
    "# * Rinse and Repeat from Step 1 (cognitive churn management is a continuous process and not once a year exercise).\n",
    "\n",
    "# Customer churn refers to when a customer (player, subscriber, user, etc.) ceases his or her relationship with a company. Online businesses typically treat a customer as churned once a particular amount of time has elapsed since the customer’s last interaction with the site or service. The full cost of customer churn includes both lost revenue and the marketing costs involved with replacing those customers with new ones. Reducing customer churn is a key business goal of every online business.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine Business Objective\n",
    "\n",
    "*Background:*\n",
    "\n",
    "Customer churn refers to when a customer ceases his or her relationship with a company. According to the authors of “Leading on the Edge of Chaos”, a 2% increase in customer retention (or decreasing churn) is equivalent to 10% reduction in costs. Additionally as per the White House Office of Consumer Affairs, it is 6–7 times more expensive to acquire a new customer than to retain an old one.\n",
    "\n",
    "All businesses in the consumer market and enterprise sectors have to deal with churn as it could end up affecting the company revenue numbers and thereby influence policy decisions. Churn could happen due to many different reasons and churn analysis helps to identify the cause (and timing) of it thereby opening up opportunities to implement effective retention strategies.\n",
    "\n",
    "The full cost of customer churn includes both lost revenue and the marketing costs involved with replacing those customers with new ones. Reducing customer churn is a key business goal of every online business.\n",
    "\n",
    "\n",
    "*Business goals:* \n",
    "\n",
    "The ability to predict that a particular customer is at a high risk of churning, while there is still time to do something about it, represents a huge additional potential revenue source for every online business. Besides the direct loss of revenue that results from a customer abandoning the business, the costs of initially acquiring that customer may not have already been covered by the customer’s spending to date. (In other words, acquiring that customer may have actually been a losing investment.) Furthermore, it is always more difficult and expensive to acquire a new customer than it is to retain a current paying customer.\n",
    "\n",
    "\n",
    "*Business success criteria:* \n",
    "\n",
    "To build and interpret a classification model that uses Logistic Regression to predict voluntary and involuntary customer churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess Situation\n",
    "This task is not applicable for this project; however, assessments are typically comprised of the following:\n",
    "\n",
    "* Prodiving an inventory of resources (Data Managers, Technical Support, etc.)\n",
    "* Document requirements, assumptions and constraints\n",
    "* Identify risks and contingencies\n",
    "* Chronicle relevant terminology\n",
    "* Preparation of Cost-Benefit Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine Data Mining Goals\n",
    "Logistic regression predicts the probability of an outcome that can only have two values (i.e. a dichotomy). The prediction is based on the use of one or several predictors (numerical and categorical). In this case, we will attempt to predict the probability of customer churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Produce Project Plan\n",
    "This task is not applicable for this project, however we will leveraging various Python librairies to assist us with our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This phase starts with an initial data collection and proceeds with activities in order to get familiar with the data, to identify data quality problems, to discover first insights into the data, or to detect interesting subsets to form hypotheses for hidden information. Below are tasks associated with this phase:\n",
    "\n",
    "* Collect Initial Data\n",
    "* Describe Data\n",
    "* Explore Data\n",
    "* Verify Data Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect Initial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c plotly plotly=4.2.1\n",
    "# !pip install \"notebook>=5.3\" \"ipywidgets>=7.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install \"notebook>=5.3\" \"ipywidgets>=7.2\n",
    "\n",
    "# import plotly.graph_objects as go\n",
    "# fig = go.Figure(data=go.Bar(y=[2, 3, 1]))\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import qgrid\n",
    "# # t = qgrid.show_grid(df,show_toolbar=True)\n",
    "# qgrid_widget = qgrid.show_grid(df, show_toolbar=True)\n",
    "# qgrid_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install qgrid\n",
    "# !jupyter labextension install qgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbextension enable --py --sys-prefix qgrid\n",
    "# !jupyter nbextension enable --py --sys-prefix widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes\n",
    "# df.get_dtype_counts()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library & dataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# df = sns.load_dataset('iris')\n",
    " \n",
    "# Density\n",
    "sns.pairplot(df, diag_kind=\"kde\")\n",
    " \n",
    "# Histogram\n",
    "sns.pairplot(df, diag_kind=\"hist\",palette=\"Set2\")\n",
    " \n",
    "# You can custom it as a density plot or histogram so see the related sections\n",
    "sns.pairplot(df, diag_kind=\"kde\", hue = \"Churn\",diag_kws=dict(shade=True, bw=.05, vertical=False),palette=\"Set2\" )\n",
    "\n",
    "# sns.pairplot(df, kind=\"scatter\", hue=\"Churn\", markers=[\"o\", \"s\", \"D\"], palette=\"Set2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Histogram:\n",
    "# # =========================\n",
    "# df.hist(figsize=(13,10));\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Scatter Matrix:\n",
    "# # =========================\n",
    "# scatter_matrix(df, alpha=0.2, figsize=(6, 6), diagonal='kde')\n",
    "# # sns.distplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Grid:\n",
    "# =========================================\n",
    "\n",
    "# !pip install qgrid\n",
    "# !conda install -c tim_shawver qgrid\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import qgrid\n",
    "\n",
    "# qgrid_widget = qgrid.show_grid(df, show_toolbar=True)\n",
    "# qgrid_widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library & dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "ds = sns.load_dataset('iris')\n",
    " \n",
    "# left\n",
    "sns.pairplot(ds, kind=\"scatter\", hue=\"species\", markers=[\"o\", \"s\", \"D\"], palette=\"Set2\")\n",
    "plt.show()\n",
    "\n",
    "ds\n",
    " \n",
    "# right: you can give other arguments with plot_kws.\n",
    "# sns.pairplot(df, kind=\"scatter\", hue=\"species\", plot_kws=dict(s=80, edgecolor=\"white\", linewidth=2.5))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Pivot Table:\n",
    "# ==============================================\n",
    "\n",
    "# df\n",
    "# !pip install pivottablejs\n",
    "# from pivottablejs import pivot_ui\n",
    "\n",
    "# pivot_ui(df,outfile_path='pivottablejs.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall plotly\n",
    "# !conda uninstall plotly\n",
    "# !pip uninstall chart-studio\n",
    "# !conda uninstall chart-studio\n",
    "# $ conda install -c plotly plotly chart-studio\n",
    "\n",
    "# import plotly as py\n",
    "# import plotly.graph_objs as go\n",
    "# import chart_studio\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# x = np.random.randn(500)\n",
    "# data = [go.Histogram(x=x)]\n",
    "\n",
    "# py.iplot(data, filename='basic histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the following named colorscales:\n",
    "#             ['aggrnyl', 'agsunset', 'algae', 'amp', 'armyrose', 'balance',\n",
    "#              'blackbody', 'bluered', 'blues', 'blugrn', 'bluyl', 'brbg',\n",
    "#              'brwnyl', 'bugn', 'bupu', 'burg', 'burgyl', 'cividis', 'curl',\n",
    "#              'darkmint', 'deep', 'delta', 'dense', 'earth', 'edge', 'electric',\n",
    "#              'emrld', 'fall', 'geyser', 'gnbu', 'gray', 'greens', 'greys',\n",
    "#              'haline', 'hot', 'hsv', 'ice', 'icefire', 'inferno', 'jet',\n",
    "#              'magenta', 'magma', 'matter', 'mint', 'mrybm', 'mygbm', 'oranges',\n",
    "#              'orrd', 'oryel', 'peach', 'phase', 'picnic', 'pinkyl', 'piyg',\n",
    "#              'plasma', 'plotly3', 'portland', 'prgn', 'pubu', 'pubugn', 'puor',\n",
    "#              'purd', 'purp', 'purples', 'purpor', 'rainbow', 'rdbu', 'rdgy',\n",
    "#              'rdpu', 'rdylbu', 'rdylgn', 'redor', 'reds', 'solar', 'spectral',\n",
    "#              'speed', 'sunset', 'sunsetdark', 'teal', 'tealgrn', 'tealrose',\n",
    "#              'tempo', 'temps', 'thermal', 'tropic', 'turbid', 'twilight',\n",
    "#              'viridis', 'ylgn', 'ylgnbu', 'ylorbr', 'ylorrd']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import qgrid\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import plotly.graph_objects as go\n",
    "import plotly as py\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from sklearn import preprocessing, datasets, linear_model, metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "from sklearn.linear_model import *\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from pandas.plotting import scatter_matrix\n",
    "from pivottablejs import pivot_ui\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import csv file and view first 5 rows:\n",
    "\n",
    "df = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, the Telco Customer Chrun dataset will be utilized; it is in the form of a .csv file. Below are descriptions of the columns within the dataset:\n",
    "\n",
    "* **customerID** - Unique Customer Identifier\n",
    "* **gender** - Customer gender\n",
    "* **SeniorCitizen** -  Binary Flag\n",
    "* **Dependents** -  Flsg to desginate dependents in the house\n",
    "* **tenure** -  Number of years as a customer\n",
    "* **PhoneService** -  Flag to determine if Phone Service in the house\n",
    "* **MultipleLines** -  Flag to determine number of phone lines in the house\n",
    "* **InternetService** -  Type of Internet Service\n",
    "* **OnlineSecurity** -  Flag to determine if online security  in the house\n",
    "* **OnlineBackup** - Flag to determine if online backup exists in the house\n",
    "* **DeviceProtection** - Flag that determines if device protection exists in the house\n",
    "* **TechSupport** - Flag to determine if tech support exists in the house\n",
    "* **StreamingTV** - Flag to determine if Streaming TV exists in the house\n",
    "* **StreamingMovies** - Flag to determine if Streaming Movies exist in the house\n",
    "* **Contract** - Contract Type for customer\n",
    "* **PaperlessBilling** - Flag to determine if customer signed up for Paperless Billing\n",
    "* **PaymentMethod** - Payment Method for customer\n",
    "* **MonthlyCharges** - Monthly Charges for customer\n",
    "* **TotalCharges** - Total Charges for customer\n",
    "* **Churn** - Flag to determine Customer Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore Data\n",
    "The goal of this task is to examine the data within all datasets more closely. We will look at the range of values for each variable and their distributions. This should allow us to get familiar with data, spot signs of data quality problems and set the stage for data preperation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the first 5 rows for each dataset:\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate descriptive statistics that summarize the central tendency,\n",
    "# dispersion and shape of a dataset's distribution, excluding \"NaN\" values:\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print (\"Table Information:\")\n",
    "print (\"==================\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Data Type Counts:\"),\n",
    "print (\"==================\"),\n",
    "df.get_dtype_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of the dataset:\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plots:\n",
    "\n",
    "sns.pairplot(df, kind=\"scatter\", hue=\"Churn\",  palette=\"husl\")\n",
    "# plt.show()\n",
    "\n",
    "plt.suptitle('Pair Plot of Customer Churn', \n",
    "             size = 16, y=1.08);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the index values\n",
    "df.index.values\n",
    "# Check if a certain index exists\n",
    "'foo' in df.index.values\n",
    "\n",
    "# If index does not exist\n",
    "# df.set_index('column_name_to_use', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Verify Data Quality\n",
    "During this portion of the analysis, I derived the following:\n",
    "\n",
    "* Customer ID is the only unique field in the dataset\n",
    "* No index exists on the dataset\n",
    "* 7,042 rows exist with 21 columns\n",
    "* Of the 21 columns, 18 are objects, 2 are integers and 1 is float\n",
    "* The Total Charges column has been cast as an object instead of a float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data preparation phase covers all activities to construct the final datasets from the initial raw data. These activities include the following:\n",
    "\n",
    "* Select Data\n",
    "* Clean Data\n",
    "* Construct Data\n",
    "* Integrate Data\n",
    "* Format Data\n",
    "\n",
    "Data preparation is 80% of the process. The two core activities in this phase are \n",
    "Data Wrangling and Data Analysis; they are the first logical programming steps. Data Wrangling is cyclical in nature and is language/framwork independent, so it will be necessary revisit the steps multiple times.\n",
    "\n",
    "We will perform syntactical and meaningful checks on the data and identify any issues and recommend potential fixes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Data\n",
    "This task invoves the follwing steps:\n",
    "\n",
    "* Check for missing data/impute values\n",
    "* Check for duplicates\n",
    "* Check for extraneous values\n",
    "* Drop columns (if necessary)\n",
    "* Drop rows (if necessary)\n",
    "\n",
    "First, we will check for missing data by executing the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert any blank values to NaN:\n",
    "\n",
    "df.replace(\" \", np.nan, inplace=True)\n",
    "\n",
    "# Find NaN value counts in dataset:\n",
    "\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our inquiries have confirmed that 11 null values exist in the Total Charges column, so now we will examine the rows where they exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find number of records where Total Charges equal NaN:\n",
    "\n",
    "df[df['TotalCharges'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine what percentage of rows in the TotalCharges column contain missing values\n",
    "# Print out the number of unique values in this column\n",
    "\n",
    "print('Percentage of Missing Total Charges Values:', round((len(df[df.TotalCharges.isna()])/ len(df)),4)*100,'%')\n",
    "print('Number of Unique Total Charges Values Excluding Nulls:', df.TotalCharges.nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will determine the uniqueness of each column and their associated values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check uniqueness of columns in dataset:\n",
    "\n",
    "for i in df:\n",
    "    print('{} is unique: {}'.format(i, df[i].is_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values of each column:\n",
    "\n",
    "for i in df:\n",
    "    print(i,(df[i].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find min and max values for all columns:\n",
    "\n",
    "data = df.fillna('')  # Fill NA/NaN values using the specified method.\n",
    "\n",
    "for i in data:\n",
    "    print(i,(min(data[i].unique())),(max(data[i].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the contents of the object columns, having duplicate values is valid.\n",
    "\n",
    "Now we will check for extraneous values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check for extraneous values:\n",
    "\n",
    "for col in df.columns:\n",
    "    print(col, '\\n', df[col].value_counts(normalize=True).head(), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will address dropping rows and/or columns after we've finished formatting out data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constructing Data\n",
    "\n",
    "There is no need to add new fields/rows any of the datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Integrating Data\n",
    "\n",
    "We did not have any disparate datasets to add."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting Data\n",
    "\n",
    "1.) Convert the TotalCharges column to float datatype\n",
    "\n",
    "2.) Based on the earlier inquiries, we will modify “No internet service” to “No” for the following columns:\n",
    "\n",
    "* OnlineSecurity\n",
    "* OnlineBackup\n",
    "* DeviceProtection\n",
    "* TechSupport\n",
    "* streamingTV\n",
    "* streamingMovies\n",
    "\n",
    "3.) We will modify “No phone service” to “No” for column “MultipleLines”\n",
    "\n",
    "4.) We will replace the binary values of 0 and 1 for the SeniorCitizen column to \"No and \"Yes\" respectively.\n",
    "\n",
    "5.) Since the minimum tenure is 1 month and maximum tenure is 72 months, we will group them into the following groups: \n",
    "\n",
    "* 0–12 Months\n",
    "* 12–24 Months\n",
    "* 24–48 Months\n",
    "* 48–60 Months\n",
    "* Greater Than 60 Months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.) Convert Total Charges to float data type:\n",
    "\n",
    "df[\"TotalCharges\"] = df[\"TotalCharges\"].astype(float)\n",
    "\n",
    "# 2.) Replace \"No internet service\" to \"No\" for the following columns:\n",
    "\n",
    "r = [ 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection',\n",
    "                'TechSupport','StreamingTV', 'StreamingMovies']\n",
    "\n",
    "for i in r: \n",
    "    df[i]  = df[i].replace({'No internet service' : 'No'})\n",
    "\n",
    "    \n",
    "# 3.) Replace \"No phone service\" to \"No\" for the MultipleLines column:\n",
    "\n",
    "df['MultipleLines'] = df['MultipleLines'].replace({'No phone service' : 'No'})\n",
    "\n",
    "    \n",
    "# 4.) Replace 0 and 1 values to \"No and \"Yes for the SeniorCitizen column:\n",
    "\n",
    "df[\"SeniorCitizen\"] = df[\"SeniorCitizen\"].replace({1:\"Yes\",0:\"No\"})\n",
    "\n",
    "\n",
    "# 5.) Tenure to categorical column:\n",
    "\n",
    "def tenure_lab(df) :\n",
    "    \n",
    "    if df[\"tenure\"] <= 12 :\n",
    "        return \"Tenure_0-12\"\n",
    "    elif (df[\"tenure\"] > 12) & (df[\"tenure\"] <= 24 ):\n",
    "        return \"Tenure_12-24\"\n",
    "    elif (df[\"tenure\"] > 24) & (df[\"tenure\"] <= 48) :\n",
    "        return \"Tenure_24-48\"\n",
    "    elif (df[\"tenure\"] > 48) & (df[\"tenure\"] <= 60) :\n",
    "        return \"Tenure_48-60\"\n",
    "    elif df[\"tenure\"] > 60 :\n",
    "        return \"Tenure_60_Plus\"\n",
    "    \n",
    "df[\"tenure_group\"] = df.apply(lambda df:tenure_lab(df),\n",
    "                                      axis = 1)\n",
    "# 6.) Define datasets based on Churn values; will use for visualizations:\n",
    "\n",
    "churn     = df[df[\"Churn\"] == \"Yes\"]\n",
    "not_churn = df[df[\"Churn\"] == \"No\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we will drop the following:\n",
    "\n",
    "* Drop rows where TotalCharges contain NaN values (0.16% of all rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop TotalCharges rows containing NaN values:\n",
    "\n",
    "df = df[df[\"TotalCharges\"].notnull()]\n",
    "df = df.reset_index()[df.columns]\n",
    "\n",
    "def drop_cols(columns, df):\n",
    "    return df.drop(columns, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up variables for visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telco = df.iloc[:,1:]\n",
    "telco_churn = telco[telco.Churn == \"Yes\"]\n",
    "telco_not_churn = telco[telco.Churn == \"No\"]\n",
    "telco_target = [\"Churn\"]\n",
    "telco_categorical = telco.nunique()[telco.nunique().values <= 5].keys().tolist()\n",
    "telco_categorical.remove(\"Churn\")\n",
    "telco_binary = telco.nunique()[telco.nunique().values <= 2].keys().tolist()\n",
    "telco_binary.remove(\"Churn\")\n",
    "telco_numerical = telco.nunique()[telco.nunique().values >= 10].keys().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bar plot visualizations after formatting to view Churn/Non-Churn relationships between categorical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create function to loop through categorical features:\n",
    "\n",
    "def bar_plot(column):\n",
    "    trace1 = go.Bar(\n",
    "        x = telco[column].unique().tolist(),\n",
    "        y = telco_churn[column].value_counts().tolist(),\n",
    "        name = \"Churn\",\n",
    "        marker= dict(color = 'rgba(255,12,75,0.75)')) #\"rgba(255,12,75,0.75)\"))\n",
    "    trace2 = go.Bar(\n",
    "        x = telco[column].unique().tolist(),\n",
    "        y = telco_not_churn[column].value_counts().tolist(),\n",
    "        name = \"Not Churn\",\n",
    "        marker= dict(color = \"rgba(0,12,75,0.75)\"))\n",
    "    data = [trace1,trace2]\n",
    "    layout = go.Layout(barmode = \"group\", title = \"Churn relationship with \"+column,height = 300, width = 1000);\n",
    "    fig = go.Figure(data = data,layout = layout);\n",
    "#     plt.figure(figsize=(.1,.1))\n",
    "    fig.show();\n",
    "    \n",
    "for each in telco_categorical:\n",
    "    bar_plot(each);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create box plot visualizations after formatting to view Churn/Non-Churn relationships between numerical variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create function to loop through numerical variables:\n",
    "\n",
    "def box_plot(column):    \n",
    "    trace1 = go.Box(y = telco_churn[column],\n",
    "                         name = \"Churn\",\n",
    "                         marker = dict(color = 'rgba(255,12,75,0.75)'))#'rgba(255,12,75,0.75)'))\n",
    "    trace2 = go.Box(y = telco_not_churn[column],\n",
    "                         name = \"Not Churn\",\n",
    "                         marker = dict(color = \"rgba(0,12,75,0.75)\"))\n",
    "    data = [trace1, trace2]\n",
    "    layout = go.Layout(title = \"Churn Situation for \"+column,\n",
    "                      xaxis = dict(title = column),height = 400, width = 900)\n",
    "    fig = go.Figure(data = data,layout = layout)\n",
    "    fig.show()\n",
    "    \n",
    "for each in telco_numerical:\n",
    "    box_plot(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will perform Label Encoding for the binary featues and Min-Max Scaling to the features with multiple values; this will standardize and normalize the dataset.\n",
    "\n",
    "Label Encoding refers to converting the labels into numeric form so as to convert it into the machine-readable form. Machine learning algorithms can then decide in a better way on how those labels must be operated. It is an important pre-processing step for the structured dataset in supervised learning. If the categorical feature has multiple values (more than two), Label Encoding will return different values for different classes.\n",
    "\n",
    "The approach to Z-score normalization (or standardization) for featues with multiple values is Min-Max Scaling (often also simply called “normalization”).In this approach, the data is scaled to a fixed range - usually 0 to 1. The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Label Encoding and Dummy Variables:\n",
    "\n",
    "# Set up variable for customerID column:\n",
    "Id_col     = ['customerID']\n",
    "\n",
    "# Set up variable for target column:\n",
    "target_col = [\"Churn\"]\n",
    "\n",
    "# Set up list for categorical columns:\n",
    "cat_cols   = df.nunique()[df.nunique() < 6].keys().tolist()  # enusres that number of values for each column does not exceed 6\n",
    "cat_cols   = [x for x in cat_cols if x not in target_col]    # excludes the target column of \"Churn\"\n",
    "\n",
    "# Set up list for numerical columns:\n",
    "num_cols   = [x for x in df.columns if x not in cat_cols + target_col + Id_col]\n",
    "\n",
    "# Set up list for binary columns:\n",
    "bin_cols   = df.nunique()[df.nunique() == 2].keys().tolist()\n",
    "\n",
    "# Set up list for columns with multiple values:\n",
    "multi_cols = [i for i in cat_cols if i not in bin_cols]\n",
    "\n",
    "# Encode labels with value between 0 and n_classes-1:\n",
    "le = LabelEncoder()\n",
    "for i in bin_cols :\n",
    "    df[i] = le.fit_transform(df[i])  # Fits transformer to X and y with optional parameters fit_params \n",
    "                                     # and returns a transformed version of X.\n",
    "    \n",
    "# Convert categorical variable into dummy/indicator variables for columns with multiple values:\n",
    "df = pd.get_dummies(data = df,columns = multi_cols )\n",
    "\n",
    "# Transform features by scaling each feature to a given range.\n",
    "# This MinMaxScaler estimator scales and translates each feature individually such\n",
    "# that it is in the given range on the training set, e.g. between\n",
    "# zero and one.\n",
    "\n",
    "std = MinMaxScaler()\n",
    "scaled = std.fit_transform(df[num_cols])\n",
    "scaled = pd.DataFrame(scaled,columns=num_cols)\n",
    "\n",
    "# Remove rows or columns by specifying label names and corresponding\n",
    "# axis, or by specifying directly index or column names.\n",
    "# dropping original values merging scaled values for numerical columns\n",
    "\n",
    "df_orig = df.copy()  # make copy of original dataframe\n",
    "\n",
    "df_orig['Churn_Yes'] = pd.get_dummies(features['Churn'],drop_first=True)\n",
    "\n",
    "df = df.drop(columns = num_cols,axis = 1) # drop numerical columns\n",
    "\n",
    "# Merge DataFrame or named Series objects with a database-style join.\n",
    "# The join is done on columns or indexes. If joining columns on\n",
    "# columns, the DataFrame indexes *will be ignored*. Otherwise if joining indexes\n",
    "# on indexes or indexes on a column or columns, the index will be passed on\n",
    "\n",
    "df = df.merge(scaled,left_index=True,right_index=True,how = \"left\")  # merge original dataframe with scaled version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm standardization/normalization:\n",
    "df.info()\n",
    "# df.describe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create correlation heat map to view potential multicoliniarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display Correlation Heat Map:\n",
    "\n",
    "#correlation\n",
    "correlation = df.corr()\n",
    "#tick labels\n",
    "matrix_cols = correlation.columns.tolist()\n",
    "#convert to array\n",
    "corr_array  = np.array(correlation)\n",
    "\n",
    "#Plotting\n",
    "trace = go.Heatmap(z = corr_array,\n",
    "                   x = matrix_cols,\n",
    "                   y = matrix_cols,\n",
    "                   colorscale = \"spectral\", #Magma\n",
    "                   colorbar   = dict(title = \"Pearson Correlation coefficient\",\n",
    "                                     titleside = \"right\"\n",
    "                                    ) ,\n",
    "                  )\n",
    "\n",
    "layout = go.Layout(dict(title = \"Correlation Heatmap for Customer Variables\",\n",
    "                        autosize = False,\n",
    "                        height  = 720,\n",
    "                        width   = 800,\n",
    "                        margin  = dict(r = 100 ,l = 110,\n",
    "                                       t = 80,b = 210,\n",
    "                                      ),\n",
    "                        yaxis   = dict(tickfont = dict(size = 9)),\n",
    "                        xaxis   = dict(tickfont = dict(size = 9))\n",
    "                        \n",
    "                       ),yaxis_autorange='reversed'\n",
    "                  )\n",
    "\n",
    "data = [trace]\n",
    "fig = go.Figure(data=data,layout=layout)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the methodology, modeling techniques are now selected and applied at this point during the process.  Since some techniques have specific requirements regarding the structure of the data, so there can be a loop back to Data Preparation. The tasks are:\n",
    "\n",
    "* Select Modeling Technique\n",
    "* Generate Test Design\n",
    "* Build Model\n",
    "* Assess Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select Modeling Technique\n",
    "\n",
    "The modeling technique that we will use is dependent upon each question posed in the Business Goals section.\n",
    "\n",
    "1.) Does discount amount have a statistically significant effect on the quantity of a product in an order? If so, at what level(s) of discount?\n",
    "\n",
    "To answer this question, we will need to understand the distribution of product quantities associated with discount percentages within the Order Detail dataframe:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initial Model - scikit-learn\n",
    "\n",
    "train,test = train_test_split(df,test_size = .30 ,random_state = 150)\n",
    "\n",
    "cols    = [i for i in df.columns if i not in Id_col + target_col]\n",
    "X_train = train[cols]\n",
    "y_train = train[target_col]\n",
    "X_test  = test[cols]\n",
    "y_test  = test[target_col]\n",
    "\n",
    "\n",
    "logreg = LogisticRegression(fit_intercept=False, C=1e16)\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y_train.value_counts())\n",
    "# print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create an ROC Curve for the scikit-learn model:\n",
    "\n",
    "y_score = logreg.decision_function(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "\n",
    "\n",
    "sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "print('AUC: {}'.format(auc(fpr, tpr)))\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before add an ROC curve to the graph for the train set as well:\n",
    "\n",
    "y_test_score = logreg.decision_function(X_test)\n",
    "y_train_score = logreg.decision_function(X_train)\n",
    "\n",
    "test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\n",
    "train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n",
    "\n",
    "print('Test AUC: {}'.format(auc(test_fpr, test_tpr)))\n",
    "print('Train AUC: {}'.format(auc(train_fpr, train_tpr)))\n",
    "\n",
    "# sns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "lw = 2\n",
    "plt.plot(test_fpr, test_tpr, color='darkorange',\n",
    "         lw=lw, label='Test ROC curve')\n",
    "plt.plot(train_fpr, train_tpr, color='blue',\n",
    "         lw=lw, label='Train ROC curve')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the precision, recall, accuracy, and F1-score of classifier:\n",
    "\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "y_hat_train = logreg.predict(X_train)\n",
    "\n",
    "print('Training Precision: ', precision_score(y_hat_train, y_train))\n",
    "print('Testing Precision: ', precision_score(y_hat_test, y_test))\n",
    "print('\\n')\n",
    "\n",
    "print('Training Recall: ', recall_score(y_hat_train, y_train))\n",
    "print('Testing Recall: ', recall_score(y_hat_test, y_test))\n",
    "print('\\n')\n",
    "\n",
    "print('Training Accuracy: ', accuracy_score(y_hat_train, y_train))\n",
    "print('Testing Accuracy: ', accuracy_score(y_hat_test, y_test))\n",
    "print('\\n')\n",
    "\n",
    "print('Training F1-Score: ',f1_score(y_hat_train,y_train))\n",
    "print('Testing F1-Score: ',f1_score(y_hat_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the coefficient weights of your model to that generated by sci-kit learn:\n",
    "\n",
    "# print(\"Sci-kit learn's weights:\", logreg.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compare a few different regularization performances on the dataset:\n",
    "\n",
    "C_param_range = [0.001,0.01,0.1,1,10,100]\n",
    "names = [0.001,0.01,0.1,1,10,100]\n",
    "colors = sns.color_palette(\"Set2\")\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for n, c in enumerate(C_param_range):\n",
    "    #Fit a model\n",
    "    logreg = LogisticRegression(fit_intercept = False, C = c,solver='liblinear') #Starter code\n",
    "    model_log = logreg.fit(X_train, y_train)\n",
    "    print(model_log) #Preview model params\n",
    "\n",
    "    #Predict\n",
    "    y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "    y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    \n",
    "    print('AUC for {}: {}'.format(names[n], auc(fpr, tpr)))\n",
    "    print('\\n')\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color=colors[n],\n",
    "             lw=lw, label='ROC curve Normalization Weight: {}'.format(names[n]))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a general function that plots the confusion matrix:\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    #Add Normalization Option\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix without normalization:\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_hat_test, y_test)\n",
    "plot_confusion_matrix(cnf_matrix, classes=[0,1], cmap = \"cool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix:\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=[0,1], normalize=True,\n",
    "                      title='Normalized confusion matrix',cmap = \"BuPu\") # cool/YlOrRd\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets:\n",
    "\n",
    "X = df[df.columns.difference(['customerID','Churn'])]\n",
    "y = df.Churn\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate and then plot the precision, recall, accuracy, and F1-score for the test and train splits \n",
    "# using different training set sizes:\n",
    "\n",
    "training_Precision = []\n",
    "testing_Precision = []\n",
    "training_Recall = []\n",
    "testing_Recall = []\n",
    "training_Accuracy = []\n",
    "testing_Accuracy = []\n",
    "training_F1 = []\n",
    "testing_F1 = []\n",
    "\n",
    "for i in range(10,95):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=i/100.0)\n",
    "    logreg = LogisticRegression(fit_intercept = False, C = 1e12,solver='liblinear')\n",
    "    model_log = logreg.fit(X_train, y_train)\n",
    "    y_hat_test = logreg.predict(X_test)\n",
    "    y_hat_train = logreg.predict(X_train)\n",
    "\n",
    "    training_Precision.append(precision_score(y_hat_train, y_train))\n",
    "    testing_Precision.append(precision_score(y_hat_test, y_test))\n",
    "    training_Recall.append(recall_score(y_hat_train, y_train))\n",
    "    testing_Recall.append(recall_score(y_hat_test, y_test))\n",
    "    training_Accuracy.append(accuracy_score(y_hat_train, y_train))\n",
    "    testing_Accuracy.append(accuracy_score(y_hat_test, y_test))\n",
    "    training_F1.append(f1_score(y_hat_train,y_train))\n",
    "    testing_F1.append(f1_score(y_hat_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create scatter plot looking at the test and train precision:\n",
    "\n",
    "sns.set_style(\"white\", {\"axes.facecolor\": \"1\"})\n",
    "plt.scatter(list(range(10,95)), training_Precision, label = 'training_Precision',c=\"blue\")\n",
    "plt.scatter(list(range(10,95)), testing_Precision, label = 'testing_Precision', c=\"magenta\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot looking at the test and train recall:\n",
    "\n",
    "sns.set_style(\"white\", {\"axes.facecolor\": \"1\"})\n",
    "plt.scatter(list(range(10,95)), training_Recall, label = 'training_Recall',c=\"coral\")\n",
    "plt.scatter(list(range(10,95)), testing_Recall, label = 'testing_Recall',c=\"darkgreen\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot looking at the test and train accuracy:\n",
    "\n",
    "sns.set_style(\"white\", {\"axes.facecolor\": \"1\"})\n",
    "plt.scatter(list(range(10,95)), training_Accuracy, label = 'training_Accuracy',c=\"maroon\")\n",
    "plt.scatter(list(range(10,95)), testing_Accuracy, label = 'testing_Accuracy', c=\"tomato\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot looking at the test and train f1-score:\n",
    "\n",
    "sns.set_style(\"white\", {\"axes.facecolor\": \"1\"})\n",
    "plt.scatter(list(range(10,95)), training_F1, label = 'training_F1', c=\"indigo\")\n",
    "plt.scatter(list(range(10,95)), testing_F1, label = 'testing_F1', c=\"plum\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the SMOTE class from the imblearn package in order to improve the model's \n",
    "# performance on the minority class:\n",
    "\n",
    "print ('Original class distribution')\n",
    "print(y_train.value_counts()) #Previous original class distribution\n",
    "print ('\\n')\n",
    "print ('Sample class distribution')\n",
    "X_train_resampled, y_train_resampled = SMOTE().fit_sample(X_train, y_train) \n",
    "print(pd.Series(y_train_resampled).value_counts()) #Preview synthetic sample class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now let's compare a few different regularization performances on the dataset:\n",
    "\n",
    "C_param_range = [0.005, 0.1, 0.2, 0.5, 0.8, 1, 1.25, 1.5, 2]\n",
    "names = [0.005, 0.1, 0.2, 0.5, 0.8, 1, 1.25, 1.5, 2]\n",
    "colors = sns.color_palette(\"Set2\", n_colors=len(names))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for n, c in enumerate(C_param_range):\n",
    "    #Fit a model\n",
    "    logreg = LogisticRegression(fit_intercept = False, C = c, solver='liblinear') #Starter code\n",
    "    model_log = logreg.fit(X_train_resampled, y_train_resampled)\n",
    "    print(model_log) #Preview model params\n",
    "\n",
    "    #Predict\n",
    "    y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "    y_score = logreg.fit(X_train_resampled, y_train_resampled).decision_function(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    \n",
    "    print('AUC for {}: {}'.format(names[n], auc(fpr, tpr)))\n",
    "    print('\\n')\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color=colors[n],\n",
    "             lw=lw, label='ROC curve Regularization Weight: {}'.format(names[n]))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_hat_test, y_test)\n",
    "plot_confusion_matrix(cnf_matrix, classes=[0,1], cmap = \"Oranges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Describe what is misleading about the AUC score and ROC curves produced by this code:\n",
    "\n",
    "print ('Original class distribution')\n",
    "print(y.value_counts()) #Previous original class distribution\n",
    "X_resampled, y_resampled = SMOTE().fit_sample(X, y) \n",
    "print('\\n')\n",
    "print ('Sample class distribution')\n",
    "print(pd.Series(y_resampled).value_counts()) #Preview synthetic sample class distribution\n",
    "print('\\n')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=0)\n",
    "\n",
    "# Now let's compare a few different regularization performances on the dataset:\n",
    "\n",
    "C_param_range = [0.005, 0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8]\n",
    "names = [0.005, 0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "colors = sns.color_palette(\"Set2\", n_colors=len(names))\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "for n, c in enumerate(C_param_range):\n",
    "    #Fit a model\n",
    "    logreg = LogisticRegression(fit_intercept = False, C = c, solver='liblinear') #Starter code\n",
    "    model_log = logreg.fit(X_train, y_train)\n",
    "    print(model_log) #Preview model params\n",
    "\n",
    "    #Predict\n",
    "    y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "    y_score = logreg.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    \n",
    "    print('AUC for {}: {}'.format(names[n], auc(fpr, tpr)))\n",
    "    print('\\n')\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color=colors[n],\n",
    "             lw=lw, label='ROC curve Normalization Weight: {}'.format(names[n]))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "plt.yticks([i/20.0 for i in range(21)])\n",
    "plt.xticks([i/20.0 for i in range(21)])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create New Model\n",
    "# logregi = LogisticRegression(fit_intercept=True, C=1e16)\n",
    "# logregi.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# #Initial Model Plots\n",
    "# test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_hat_test)\n",
    "# train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_hat_train)\n",
    "\n",
    "\n",
    "# print('Custom Model Test AUC: {}'.format(auc(test_fpr, test_tpr)))\n",
    "# print('Custome Model Train AUC: {}'.format(auc(train_fpr, train_tpr)))\n",
    "\n",
    "# plt.figure(figsize=(10,8))\n",
    "# lw = 2\n",
    "# plt.plot(test_fpr, test_tpr, color='darkorange',\n",
    "#          lw=lw, label='Custom Model Test ROC curve')\n",
    "# plt.plot(train_fpr, train_tpr, color='blue',\n",
    "#          lw=lw, label='Custom Model Train ROC curve')\n",
    "\n",
    "\n",
    "\n",
    "# #Second Model Plots\n",
    "# y_test_score = logreg.decision_function(X_test)\n",
    "# y_train_score = logreg.decision_function(X_train)\n",
    "\n",
    "# test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\n",
    "# train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n",
    "\n",
    "# print('Scikit learn Model 1 Test AUC: {}'.format(auc(test_fpr, test_tpr)))\n",
    "# print('Scikit learn Model 1 Train AUC: {}'.format(auc(train_fpr, train_tpr)))\n",
    "\n",
    "\n",
    "# plt.plot(test_fpr, test_tpr, color='yellow',\n",
    "#          lw=lw, label='Scikit learn Model 1 Test ROC curve')\n",
    "# plt.plot(train_fpr, train_tpr, color='gold',\n",
    "#          lw=lw, label='Scikit learn Model 1 Train ROC curve')\n",
    "\n",
    "\n",
    "# #Third Model Plots\n",
    "# y_test_score = logregi.decision_function(X_test)\n",
    "# y_train_score = logregi.decision_function(X_train)\n",
    "\n",
    "# test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\n",
    "# train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n",
    "\n",
    "# print('Scikit learn Model 2 with intercept Test AUC: {}'.format(auc(test_fpr, test_tpr)))\n",
    "# print('Scikit learn Model 2 with intercept Train AUC: {}'.format(auc(train_fpr, train_tpr)))\n",
    "\n",
    "\n",
    "# plt.plot(test_fpr, test_tpr, color='purple',\n",
    "#          lw=lw, label='Scikit learn Model 2 with intercept Test ROC curve')\n",
    "# plt.plot(train_fpr, train_tpr, color='red',\n",
    "#          lw=lw, label='Scikit learn Model 2 with intercept Train ROC curve')\n",
    "\n",
    "# #Formatting\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.yticks([i/20.0 for i in range(21)])\n",
    "# plt.xticks([i/20.0 for i in range(21)])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic (ROC) Curve')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altering the Regularization Parameter\n",
    "\n",
    "# Create different subplots with varying regularization (C) parameters\n",
    "# For each, plot the ROC curve of the train and test set for that specific model\n",
    "# Regularization parameters between 1 and 20 are recommended\n",
    "\n",
    "sns.set_style(\"dark\", {\"axes.facecolor\": \".9\"})\n",
    "fig, axes = plt.subplots(4,2, figsize=(15,15))\n",
    "for n in range(8):\n",
    "    i = n%4\n",
    "    j = n//4\n",
    "    ax = axes[i,j]\n",
    "    #Fit a model\n",
    "    logreg = LogisticRegression(fit_intercept=True, C=1.5**(n))\n",
    "    logreg.fit(X_train, y_train)\n",
    "    #Print Stats\n",
    "    y_test_score = logreg.decision_function(X_test)\n",
    "    y_train_score = logreg.decision_function(X_train)\n",
    "\n",
    "    test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)\n",
    "    train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)\n",
    "    \n",
    "    test_auc = auc(test_fpr, test_tpr)\n",
    "    train_auc = auc(train_fpr, train_tpr)\n",
    "    auc_diff = round(train_auc - test_auc, 4)\n",
    "\n",
    "#     print('Test AUC with C=1.5^{}: {}'.format(n*2, auc(test_fpr, test_tpr)))\n",
    "#     print('Train AUCwith C=1.5^{}: {}'.format(n*2, auc(train_fpr, train_tpr)))\n",
    "    # Add the plot\n",
    "    ax.plot(test_fpr, test_tpr, color='orangered',\n",
    "         lw=lw, label='Test ROC curve')\n",
    "    ax.plot(train_fpr, train_tpr, color='teal',\n",
    "             lw=lw, label='train ROC curve')\n",
    "    \n",
    "    ax.set_title('Regularization Parameter set to: 1.5^{}\\nDifference in Test/Train AUC: {}'.format(n, auc_diff))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# telco = df.iloc[:,1:]\n",
    "# telco_churn = telco[telco.Churn == \"Yes\"]\n",
    "# telco_not_churn = telco[telco.Churn == \"No\"]\n",
    "# telco_target = [\"Churn\"]\n",
    "# telco_categorical = telco.nunique()[telco.nunique().values <= 5].keys().tolist()\n",
    "# telco_categorical.remove(\"Churn\")\n",
    "# telco_binary = telco.nunique()[telco.nunique().values <= 2].keys().tolist()\n",
    "# telco_binary.remove(\"Churn\")\n",
    "# telco_numerical = telco.nunique()[telco.nunique().values >= 10].keys().tolist()\n",
    "telco.head()\n",
    "# telco['Churn']\n",
    "# telco_target\n",
    "# telco_categorical\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "df1.head()\n",
    "df.head()\n",
    "\n",
    "# # obtaining the X and Y variables from the dataframe\n",
    "features = telco[telco.columns.difference(['customerID'])]\n",
    "target = telco['Churn']\n",
    "# # creating dummy variable for Churn\n",
    "features['Churn_Yes'] = pd.get_dummies(features['Churn'],drop_first=True)\n",
    "features.drop(columns=['Churn'], inplace=True)\n",
    "features.head()\n",
    "# features.info()\n",
    "\n",
    "# features[['Churn','Churn_Yes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=20,test_size=0.2)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "\n",
    "## scaling every feature except the binary column female\n",
    "scaler.fit(X_train.iloc[:,:-1])\n",
    "transformed_training_features = scaler.transform(X_train.iloc[:,:-1])\n",
    "transformed_testing_features = scaler.transform(X_test.iloc[:,:-1])\n",
    "\n",
    "X_train_transformed = pd.DataFrame(scaler.transform(X_train.iloc[:,:-1]), columns=X_train.columns[:-1], index=X_train.index)\n",
    "X_train_transformed['Churn_Yes'] = X_train['Churn_Yes']\n",
    "\n",
    "X_test_transformed = pd.DataFrame(scaler.transform(X_test.iloc[:,:-1]), columns=X_train.columns[:-1], index=X_test.index)\n",
    "X_test_transformed['Churn_Yes'] = X_test['Churn_Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def run_model(model,X_train,X_test,y_train,y_test):\n",
    "    \n",
    "#     print('Training R^2 :',model.score(X_train,y_train))\n",
    "#     y_pred_train = model.predict(X_train)\n",
    "#     print('Training Root Mean Square Error',np.sqrt(metrics.mean_squared_error(y_train,y_pred_train)))\n",
    "#     print('\\n----------------\\n')\n",
    "#     print('Testing R^2 :',model.score(X_test,y_test))\n",
    "#     y_pred_test = model.predict(X_test)\n",
    "#     print('Testing Root Mean Square Error',np.sqrt(metrics.mean_squared_error(y_test,y_pred_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lm = LinearRegression()\n",
    "# lm.fit(X_train_transformed,y_train)\n",
    "# run_model(lm,X_train_transformed,X_test_transformed,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFE\n",
    "\n",
    "# logit = LogisticRegression()\n",
    "\n",
    "# rfe = RFE(logit,8)\n",
    "# rfe = rfe.fit(X_resampled,y_resampled.values.ravel())\n",
    "\n",
    "# rfe.support_\n",
    "# rfe.ranking_\n",
    "\n",
    "# # identified columns Recursive Feature Elimination\n",
    "# idc_rfe = pd.DataFrame({\"rfe_support\" :rfe.support_,\n",
    "#                        \"columns\" : [i for i in telcom.columns if i not in Id_col + target_col],\n",
    "#                        \"ranking\" : rfe.ranking_,\n",
    "#                       })\n",
    "# cols = idc_rfe[idc_rfe[\"rfe_support\"] == True][\"columns\"].tolist()\n",
    "\n",
    "\n",
    "# # separating train and test data:\n",
    "# train_rf_X = X_resampled[cols]\n",
    "# train_rf_Y = y_resampled\n",
    "# test_rf_X  = X_test[cols]\n",
    "# test_rf_Y  = Y_test[target_col]\n",
    "\n",
    "# logit_rfe = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "#           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "#           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "#           verbose=0, warm_start=False)\n",
    "\n",
    "# #applying model\n",
    "# telecom_churn_prediction(logit_rfe,train_rf_X,test_rf_X,train_rf_Y,test_rf_Y,\n",
    "#                          cols,\"coefficients\",threshold_plot = True)\n",
    "\n",
    "# tab_rk = ff.create_table(idc_rfe)\n",
    "# py.iplot(tab_rk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Minority Oversampling Technique (SMOTE)\n",
    "\n",
    "# cols    = [i for i in df.columns if i not in Id_col+target_col]\n",
    "\n",
    "# smote_X = df[cols]\n",
    "# smote_Y = df[target_col]\n",
    "\n",
    "# #Split train and test data\n",
    "# smote_train_X,smote_test_X,smote_train_Y,smote_test_Y = train_test_split(smote_X,smote_Y,\n",
    "#                                                                          test_size = .25 ,\n",
    "                                                                         random_state = 111)\n",
    "\n",
    "# #oversampling minority class using smote\n",
    "# os = SMOTE(random_state = 0)\n",
    "# os_smote_X,os_smote_Y = os.fit_sample(smote_train_X,smote_train_Y)\n",
    "# os_smote_X = pd.DataFrame(data = os_smote_X,columns=cols)\n",
    "# os_smote_Y = pd.DataFrame(data = os_smote_Y,columns=target_col)\n",
    "# ###\n",
    "\n",
    "\n",
    "\n",
    "# logit_smote = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "#           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "#           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "#           verbose=0, warm_start=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Test Design "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welch's t-test is more robust than Student's t-test and maintains Type I error rates close to nominal for unequal variances and for unequal sample sizes under normality. Furthermore, the power of Welch's t-test comes close to that of Student's t-test, even when the population variances are equal and sample sizes are balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohen’s d is one of the most common ways to measure effect size. As an effect size, Cohen's d is typically used to represent the magnitude of differences between two (or more) groups on a given variable, with larger values representing a greater differentiation between the two groups on that variable.\n",
    "\n",
    "The general “rule of thumb” guidelines for Cohen's d is as follows:\n",
    "\n",
    "* Small effect = 0.2\n",
    "\n",
    "* Medium Effect = 0.5\n",
    "\n",
    "* Large Effect = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 5 steps to execute the Welch's t-test:\n",
    "\n",
    "* Set up null and alternative hypotheses\n",
    "* Choose a significance level\n",
    "* Set up Cohen's d function to determine effect\n",
    "* Conduct the Welch's t-test\n",
    "* Determine the p-value (find the rejection region)\n",
    "* Accept or reject the Null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Set up null and alternative hypotheses\n",
    "\n",
    "$H_O$: Discounts have no effect on the number of products customers order.<br> \n",
    "$H_a$: Discounts have an effect on the number of products customers order.<br> \n",
    "\n",
    "\n",
    "##### * Choose a significance level\n",
    "\n",
    "Our significance level, or $\\alpha$ = 0.05. \n",
    "If p < $\\alpha$, we reject the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Cohen's d function:\n",
    "\n",
    "def Cohen_d(group1, group2):\n",
    "    diff = group1.mean() - group2.mean()\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    var1 = group1.var()\n",
    "    var2 = group2.var()\n",
    "    pooled_var = (n1 * var1 + n2 * var2) / (n1 + n2)\n",
    "    d = diff / np.sqrt(pooled_var)\n",
    "    return abs(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### * Conduct the Welch's t-test\n",
    "##### * Determine the p-value\n",
    "##### * Accept or reject the Null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct Welch's t- test:\n",
    "\n",
    "from scipy import stats \n",
    "no_disc = dfdisc[dfdisc['Discount']==0]['Quantity']  # control group\n",
    "disc = dfdisc[dfdisc['Discount']!=0]['Quantity']     # experimental group  \n",
    "\n",
    "'''It is important to keep the discounted items and non-discounted items seperate. We will\n",
    "refer to the non-discounted items as the control. This prevents any undue influence.'''\n",
    "\n",
    "t_stat, p = stats.ttest_ind(no_disc, disc)   # welchs t-test \n",
    "d = Cohen_d(disc, no_disc)\n",
    "print('p_value =', p)\n",
    "print('Reject null hypothesis') if p < 0.05 else print('Failed to reject null hypothesis')\n",
    "print(\"Cohen's d =\", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess Model\n",
    "Since the p-value is less than the selected significance level, we can reject the null hypothesis.\n",
    "In addition, the Cohen's d value suggests that effect of discounts on order quantities is small in nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is there a statistically significant difference in discount between Categories?\n",
    "\n",
    "* Is there a statistically significant difference in performance of Shipping Companies?\n",
    "\n",
    "* Is there a statistically significant difference in performance of Suppliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer these questions, we will use ANOVA (Analysis of Variance) testing. ANOVA is a method for generalizing statistical tests to multiple groups. ANOVA analyses the overall variance of a dataset by partitioning the total sum of square of deviations (from the mean) into the sum of squares for each of these groups and sum of squares for error. By comparing the statistical test for multiple groups, it can serve as a useful alternative to 𝑡 -tests when testing multiple factors simultaneously is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-way ANOVA is an extension of the one-way ANOVA that examines the influence of two different categorical independent variables on one continuous dependent variable. The two-way ANOVA not only aims to assess the main effect of each independent variable but also to see if there is any interaction between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.) Discounts Between Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up null and alternative hypotheses\n",
    "\n",
    "- $H_0$: There is no difference in discount levels between categories\n",
    "- $H_\\alpha$: There is a difference in discount levels between categories\n",
    "\n",
    "\n",
    "##### Choose a significance level\n",
    "\n",
    "Our significance level, or $\\alpha$ = 0.05. \n",
    "If p < $\\alpha$, we reject the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catdisc = pd.read_sql_query('''\n",
    "SELECT OrderDetail.UnitPrice, Discount, CategoryId \n",
    "FROM OrderDetail\n",
    "JOIN Product\n",
    "ON OrderDetail.ProductId = Product.Id\n",
    "''',conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols = Create a Model from a formula and dataframe\n",
    "\n",
    "formula = 'Discount ~ C(CategoryId)'  # The formula specifying the model\n",
    "lm = ols(formula, catdisc).fit()   # The data for the model\n",
    "table = sm.stats.anova_lm(lm, typ=2) \n",
    "# Anova table for one or more fitted linear models\n",
    "# lm = model, typ=2 indicates type of ANOVA test\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANOVA Definitions:\n",
    "* sum_sq = the sum of squares due to the source\n",
    "* df = the degrees of freedom in the source\n",
    "* F = the F-statistic; variance of the group means (Mean Square Between) / mean of the within group variances (Mean Squared Error)\n",
    "* PR(>F) = probability of getting a given F-statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results show that there is no _**statistically significant**_ difference in discount level between Categories, therefore we cannot reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.)  Shipping Companies' Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up null and alternative hypotheses\n",
    "\n",
    "- $H_0$: There is no difference in performance of Shipping Companies\n",
    "- $H_\\alpha$: There is a difference in performance of Shipping Companies\n",
    "\n",
    "\n",
    "##### Choose a significance level\n",
    "\n",
    "Our significance level, or $\\alpha$ = 0.05. \n",
    "If p < $\\alpha$, we reject the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert argument to datetime:\n",
    "\n",
    "dfOrder.OrderDate = pd.to_datetime(dfOrder.OrderDate)\n",
    "dfOrder.ShippedDate = pd.to_datetime(dfOrder.ShippedDate)\n",
    "dfOrder.RequiredDate = pd.to_datetime(dfOrder.RequiredDate)\n",
    "\n",
    "# Calculate Processing and Shipping Time:\n",
    "\n",
    "dfOrder['ProcessingTime'] = dfOrder.ShippedDate - dfOrder.OrderDate\n",
    "dfOrder['ShippingTime'] = dfOrder.RequiredDate - dfOrder.ShippedDate\n",
    "\n",
    "# Convert to number of days:\n",
    "\n",
    "dfOrder.ShippingTime = dfOrder.ShippingTime.dt.days\n",
    "dfOrder.ProcessingTime = dfOrder.ProcessingTime.dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# View results of table:\n",
    "\n",
    "dfOrder.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review the mean for all values:\n",
    "\n",
    "dfOrder.groupby('ShipVia').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'ProcessingTime ~ C(ShipVia)'\n",
    "lm = ols(formula, dfOrder).fit()\n",
    "table = sm.stats.anova_lm(lm, typ=2)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results show that there is a _**statistically significant**_ difference in Shipping Compnaies, therefore we can reject the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.)  Supplier Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up null and alternative hypotheses\n",
    "\n",
    "- $H_0$: There is no difference in performance of Suppliers\n",
    "- $H_\\alpha$: There is a difference in performance of Suppliers\n",
    "\n",
    "\n",
    "##### Choose a significance level\n",
    "\n",
    "Our significance level, or $\\alpha$ = 0.05. \n",
    "If p < $\\alpha$, we reject the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = pd.read_sql_query(\"\"\"\n",
    "SELECT CompanyName, a.Quantity\n",
    "FROM OrderDetail a\n",
    "JOIN Product b ON a.ProductID = b.ID\n",
    "JOIN Supplier c ON b.SupplierID = c.ID\n",
    "\"\"\",conn)\n",
    "# 51317"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'Quantity ~ C(CompanyName)'\n",
    "lm = ols(formula, reg).fit()\n",
    "table = sm.stats.anova_lm(lm, typ=2)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test results show that there is no _**statistically significant**_ difference in Supplier performance, therefore we cannot reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
